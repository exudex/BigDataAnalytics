{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A10: Big Data Applications using Map Reduce \n",
    "##### by Kevin Nguyen (17 March 2017)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment we will be designing a Page Rank algorithm utilizing mapreduce in the cloudera HDFS environment via RHadoop. Page Rank is ranking webpages in order of importance and quality. The assumption is that webpages with more links from other webpages are more important. Page Rank is the algorithm used in Googleâ€™s search engine that helps determine what websites are shown to the user. \n",
    "\n",
    "The following instructions were followed for this project.\n",
    "\n",
    "1.\tDesign a Map Reduce method to design the (key, val) structure for the page rank equation (the original version). \n",
    "2.\tWrite a code (either in R or another language) for implement the Map reduce method designed in Q1. Explain your code in a separate document and provide full comments in the code. \n",
    "3.\tRun Example 5.1 in the text book and find the final page ranking to validate your code. You should use \"hand\" calculation to generate the true solution to validate the outcome. The code must be functional to earn full credit. \n",
    "4.\tRun your code based on the web-Google.txt file. You need to compute the initial m(i,j) based on the data set first. show the top 10 highest ranked pages. The code must be functional to earn full credit.  \n",
    "\n",
    "## 1.Map Reduce Design Strategy \n",
    "\n",
    "The original page rank equation is v' = M * v. By running through mutliple iterations your page rank vector v will start converging to a 'final' page rank vector as you replace v with v'(v' = new page rank, v = current page rank) after each iteration.  There is no single 'correct' final page rank for any problem, the final page rank can change/differ due to the amount of iterations used or where you start your initial page rank vector. For a 'big data' file, 50-75 iterations are reccomended to get a good estimate of the final page rank vector. On the other hand, 'small data' files will converge very quickly and then you can pass the true final page rank (from a mathematical point of view) vector after a handful of iterations. \n",
    "\n",
    "Our map reduce strategy using Rhadoop will be documented below. \n",
    "\n",
    "v' = M * v : a map reduce code to iteratively compute v=M * v where columuns of M adds to 1\n",
    "\n",
    "             M is n x n and v is n x 1\n",
    "             M is in the format of (i, j, mij), sum(mij)=1 over j           \n",
    "Example 5.1 from the Mining Massive Data Book will be the basis which we will use to help us lay the foundation for our strategy. \n",
    "![](http://localhost:8889/files/Ex5_1.png)\n",
    "$$\\mathbf{M} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0 & 1/2 & 1 & 0  \\\\\n",
    "1/3 & 0 & 0 & 1/2 \\\\\n",
    "1/3 & 0 & 0 & 1/2 \\\\\n",
    "1/3 & 1/2 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "\\mathbf{v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "1/4  \\\\\n",
    "1/4 \\\\\n",
    "1/4 \\\\\n",
    "1/4\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Converted matrix M into new i j Mij format M\n",
    "$$\\mathbf{M} = \\left[\\begin{array}\n",
    "{rrr}\n",
    "i & j & Mij\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 2 & 0.5\\\\\n",
    "1 & 3 & 1\\\\\n",
    "1 & 4 & 0\\\\\n",
    "2 & 1 & 0.333\\\\\n",
    "2 & 2 & 0\\\\\n",
    "2 & 3 & 0\\\\\n",
    "2 & 4 & 0.5\\\\\n",
    "3 & 1 & 0.333\\\\\n",
    "3 & 2 & 0\\\\\n",
    "3 & 3 & 0\\\\\n",
    "3 & 4 & 0.5\\\\\n",
    "4 & 1 & 0.333\\\\\n",
    "4 & 2 & 0.5\\\\\n",
    "4 & 3 & 0\\\\\n",
    "4 & 4 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "The i j Mij format above will be stored into a text file 'M.txt'.\n",
    "\n",
    "Mapreduce is uses a (key,val) structure. Both the mapper and reducer will be used in this case.\n",
    "\n",
    "    Mapper(key,val)\n",
    "    The intial mapper key will be null and val will be the content of our M.txt\n",
    "    i is used as our new key and Mij for our new val. This effectively maps our i directly with our Mij due to our values being  stored in a usable sequence. We then pass our newly mapped(key, val) to be passed into our reducer.\n",
    "\n",
    "    Reducer(key,val)\n",
    "    The initial reducer mapper key will be i and Mij that was set in our mapper previously. The reducer will then multiply our Mij with the the vector v, the output is stored into a new v, 'vp'. Lastly, all the values in vp will be summed together based on their assigned keys. This will result in a new vector v that can continue to be reiterated upon to help converge towards a final page rank. \n",
    "\n",
    "## 2. Map Reduce Code - pageRankM.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pageRankM.r\n",
    "#Author: Kevin Nguyen\n",
    "#A10: Big Data Applications using Map Reduce\n",
    "#17 March 2017\n",
    "#\n",
    "#Template provided by Dr. Shing Chang\n",
    "#\n",
    "\n",
    "#a map reduce code to iteratively compute v=M v where columuns of M adds to 1\n",
    "#an example on section 5.2 in Mining of Massive Datasets\n",
    "#M is n x n and v is n x 1\n",
    "#M is in the format of (i, j, mij), sum(mij)=1 over j\n",
    "\n",
    "# mapper function\n",
    "\t#Note that the input part of mapreduce will provide k as NULL and v as the content of M in (i, j, mij format) \n",
    "pgij<-function(k,v){\n",
    "\t\n",
    "\t#Seperates v into usable components\n",
    "\ta<-strsplit(unlist(v), \" \")#Splits the unlisted v into subgroups \n",
    "\tsi<-as.numeric(sapply(a, \"[\", 1))#Seperate subelement used as key into vector (i)\n",
    "\tvp<-as.numeric(sapply(a, \"[\", 3))# Seperate subelement used as values into vector (mij)\n",
    "\t\n",
    "\t#The key passes into the reduce function is si and value vp\n",
    "\tkeyval(si, vp)\n",
    "}\n",
    "\n",
    "# reducer function\n",
    "pgij2<-function(si,vp){\n",
    "\t\n",
    "\t#Page Rank calculation v = M*v where v1 converges to the final page rank after multiple iterations\n",
    "\tvf<-vp*v1\n",
    "\t\n",
    "\t#Summation of values with respect to their key for final page rank vector\n",
    "\tkeyval(si, sum(vf))\n",
    "}\n",
    "\n",
    "# main program\n",
    "\n",
    "#make sure M.txt is the data file to be executed \n",
    "#input the number of unique node and assign it to n; 4 is the default for M.txt data; \n",
    "#n should be the unique count of the \"to website\" column; a word count \n",
    "#the following initial values are for Example 5.1 of the Mining Massive Data book (1 or 2nd edition)\n",
    "library(rmr2)\n",
    "\n",
    "n<-4 # Number of unique nodes\n",
    "\n",
    "#Final Page Rank once multiple iterations have processed\n",
    "v1<- rep(1/n, n) #Final Page Rank once multiple iterations have processed\n",
    "\t\t\t\t #Note that v1 is a global varialbe to be used in the while loop below and the pgij function above\n",
    "i=1\n",
    "nn<-10 #Amount of iterations, 50-75 for big data files\n",
    "input.dfs<-(\"/user/cloudera/M.txt\")\n",
    "\n",
    "# the loop to start iterations\n",
    "while (i<=nn){\n",
    "\t\n",
    "\t#MapReduce function from dfs \n",
    "\ttv<-from.dfs(mapreduce(input=input.dfs, input.format=\"text\", map=pgij, reduce = pgij2))\n",
    "\tv1<-as.numeric(tv$val) #here the output in tv is passed onto the global variable v1\n",
    "\ti <- i+1\n",
    "}\n",
    "#Save output into text file and also print in command terminal\n",
    "write.table(data.frame(tv$key,v1), \"/home/cloudera/PageRank_4I.txt\", sep= \" \")\n",
    "print(paste(\"The final printout in the $key part is \", tv$key))\n",
    "print(paste(\"The final page rank vector is \", v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a detailed explanation of the map reduce code above, please scroll down to Appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example 5.1 using pageRankM.r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pageRankM.r was used to calculate the following final page rank vector for example 5.1. The strategy for our mapreduce was based on example 5.1. Below is the expected page rank vector based on the amount of iterations performed starting at base v. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "1/4  \\\\\n",
    "1/4 \\\\\n",
    "1/4 \\\\\n",
    "1/4\n",
    "\\end{array}\\right]\n",
    "\\mathbf{1I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.375  \\\\\n",
    "0.2083333333 \\\\\n",
    "0.2083333333\\\\\n",
    "0.2083333333\n",
    "\\end{array}\\right]\n",
    "\\mathbf{2I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3125  \\\\\n",
    "0.2291666667 \\\\\n",
    "0.2291666667\\\\\n",
    "0.2291666667\n",
    "\\end{array}\\right]\n",
    "\\mathbf{3I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.34375  \\\\\n",
    "0.21875 \\\\\n",
    "0.21875\\\\\n",
    "0.21875\n",
    "\\end{array}\\right]\n",
    "\\mathbf{4I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.328125  \\\\\n",
    "0.2239583333 \\\\\n",
    "0.2239583333\\\\\n",
    "0.2239583333\n",
    "\\end{array}\\right]\n",
    "\\mathbf{5I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3359375  \\\\\n",
    "0.2213541667 \\\\\n",
    "0.2213541667\\\\\n",
    "0.2213541667\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{10I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3332519531  \\\\\n",
    "0.222249349 \\\\\n",
    "0.222249349\\\\\n",
    "0.222249349\n",
    "\\end{array}\\right]\n",
    "\\mathbf{15I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3333358765  \\\\\n",
    "0.2222213745 \\\\\n",
    "0.2222213745\\\\\n",
    "0.2222213745\n",
    "\\end{array}\\right]\n",
    "\\mathbf{20I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3333332539  \\\\\n",
    "0.2222222487 \\\\\n",
    "0.2222222487\\\\\n",
    "0.2222222487\n",
    "\\end{array}\\right]\n",
    "\\mathbf{50I v} = \\left[\\begin{array}\n",
    "{rrrr}\n",
    "0.3333333333  \\\\\n",
    "0.2222222222 \\\\\n",
    "0.2222222222\\\\\n",
    "0.2222222222\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the output of pageRankM.r ran for multiple iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageRankM.r Output\n",
    "1 Iteration\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.375\n",
    "\"2\" 2 0.20825\n",
    "\"3\" 3 0.20825\n",
    "\"4\" 4 0.20825\n",
    "2 Iterations  \n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.312375\n",
    "\"2\" 2 0.229\n",
    "\"3\" 3 0.229\n",
    "\"4\" 4 0.229\n",
    "3 Interations  \n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.3435\n",
    "\"2\" 2 0.218520875\n",
    "\"3\" 3 0.218520875\n",
    "\"4\" 4 0.218520875\n",
    "4 Iterations\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.3277813125\n",
    "\"2\" 2 0.2236459375\n",
    "\"3\" 3 0.2236459375\n",
    "\"4\" 4 0.2236459375\n",
    "5 Iterations\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.33546890625\n",
    "\"2\" 2 0.2209741458125\n",
    "\"3\" 3 0.2209741458125\n",
    "\"4\" 4 0.2209741458125\n",
    "10 Iterations\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.332235057978652\n",
    "\"2\" 2 0.221497085308783\n",
    "\"3\" 3 0.221497085308783\n",
    "\"4\" 4 0.221497085308783\n",
    "15 Iterations\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.331764893818602\n",
    "\"2\" 2 0.221100334464525\n",
    "\"3\" 3 0.221100334464525\n",
    "\"4\" 4 0.221100334464525\n",
    "20 Iterations\n",
    "\"tv.key\" \"tv.val\"\n",
    "\"1\" 1 0.331209603146945\n",
    "\"2\" 2 0.220732862072592\n",
    "\"3\" 3 0.220732862072592\n",
    "\"4\" 4 0.220732862072592\n",
    "50 Iterations\n",
    "\"tv.key\" \"v1\"\n",
    "\"1\" 1 0.327912814349848\n",
    "\"2\" 2 0.218535657185172\n",
    "\"3\" 3 0.218535657185172\n",
    "\"4\" 4 0.218535657185172"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of pageRankM.r gets close to the 'true' final page rank vector at 10 iterations. Past that point, the page rank vector begins diverging slowly. This is normal behaviour due to the data from example 5.1 being extremely small. For a big data file, 50-75 iterations would be necessary to get close to a good estimate for the final page rank value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. web.Google.txt Map Reduce with pageRankG.r\n",
    "\n",
    "For part 4 of this project. We need to identify the top 10 highest ranked pages within the provided sample file, 'web-Google.txt'. When looking at the file in terms of an i, j, and Mij. Only the i and j are provided, thus we need to create our own Mij(attempting to make the data stochastic). Column 1 is j and column 2 is i from the text file. \n",
    "\n",
    "Impala was used to count how many times the same fromnode pointed towards a different tonode. This allows us to merge the wordcount from impala with the 'web-Google.txt' file to help us try to make the data stochastic. Below is the code used in Impala and R to create the new data file we will be using mapreduce on to achieve our desired page rank. \n",
    "\n",
    "Results of pageRankG.r for 10 Iterations\n",
    "\n",
    "Top 10 Nodes and the rank \n",
    "\n",
    "1.  21208\t0.00079235046509864996\n",
    "2.  783323\t0.00060994192570862599\n",
    "3.\t563390\t0.00060446072785647596\n",
    "4.\t64511\t0.00019099534832848399\n",
    "5. \t15808\t0.00010690541083469099\n",
    "6. \t511681\t0.000106597183337524\n",
    "7. \t30681\t9.6075760286939502e-05\n",
    "9.  1497    8.8466291511110298e-05\n",
    "10. 555920\t7.6037245315841402e-05\n",
    "\n",
    "These were the top 10 pages after running 50 iterations. \n",
    "pageRankG.r is not perfect and has issues with the ranks converging to 0. This is likely due to the values of the ranks falling out of double precision values even with the revised page rank formula. 50 Iterations was attempted but produced only the following 3 page ranks. \n",
    "\n",
    "Results of pageRankG.r for 50 Iterations \n",
    "\n",
    "Top Nodes and the rank \n",
    "\n",
    "1. \t11\t2.2592664094190601e-07\n",
    "2.  9\t2.3487944489531501e-07\n",
    "3.\t7\t7.7193525429078297e-07\n",
    "\n",
    "One likely issue with pageRankG.r compared to pageRankM.r is that the vector order which the v=M*v based on j does not actually match up during the operation after the first iteration. \n",
    "\n",
    "Should webr2.txt be reordered in a way that it would match up with the vector every iteration the issue could be solved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Impala Query Code to create webg2 table\n",
    "create table webg2 as \n",
    "select fromnode, count(*) as uniqueNode from web_google group by fromnode order by fromnode;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a wordcount for fromnodes from 'web-Google.txt' table which will be used to help create mij for the data.\n",
    "The new table is then extracted into a csv from Hue to HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#webAppend.r\n",
    "#Merge two tables together based on the first column of both tables (i of i, j, mij)\n",
    "\n",
    "a<-read.table(\"/home/cloudera/web-Google.txt\", sep = \"\\t\") #i & j of i, j, mij\n",
    "b<-read.table(\"/home/cloudera/webg2.csv\", sep =\",\") #i & mij of i, j, mij\n",
    "d<-merge(a, b, by.x='V1', by.y='V1') #i, j, mij (merged based on i)\n",
    "write.table(d, \"/home/cloudera/webr2.txt\", sep= \" \", row.names = FALSE, col.names = FALSE) # Write new data file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above merges the data from 'web-Google.txt' with the new table with weights, 'webg2.csv', into a new data file called 'webr2.txt'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pageRankG.r\n",
    "#Author: Kevin Nguyen\n",
    "#A10: Big Data Applications using Map Reduce\n",
    "#30 March 2017\n",
    "#\n",
    "#Template provided by Dr. Shing Chang\n",
    "#\n",
    "\n",
    "#a map reduce code to iteratively compute v=M v where columuns of M adds to 1\n",
    "#M is composed of 3 vectors, j, i, mij \n",
    "#M is in the format of (i, j, mij), sum(mij)=1 over j\n",
    "\n",
    "# mapper function\n",
    "\t#Note that the input part of mapreduce will provide k as NULL and v as the content of M in (i, j, mij format) \n",
    "pgij<-function(k,v){\n",
    "\t\n",
    "\t#Seperates v into usable components\n",
    "\ta<-strsplit(unlist(v), \" \")#Splits the unlisted v into subgroups \n",
    "\tsj<-as.numeric(sapply(a, \"[\", 1))\n",
    "\tsi<-as.numeric(sapply(a, \"[\", 2))#Seperate subelement used as key into vector (i)\n",
    "\tvp<-as.numeric(sapply(a, \"[\", 3))# Seperate subelement used as values into vector (mij)\n",
    "\t\n",
    "\t#Page Rank calculation v = b*M*v-(1-b)e/n where v1 converges to some page rank \n",
    "\tvp2<-(b*(1/vp)*v1[si])+(1-b)/n #Revised page rank formula\n",
    "\t\n",
    "\t#The key passes into the reduce function is si and value vp\n",
    "\tkeyval(si, vp2)\n",
    "}\n",
    "\n",
    "# reducer function\n",
    "pgij2<-function(si,vp){\n",
    "\t\n",
    "\t#Summation of values with respect to their key for final page rank vector\n",
    "\tkeyval(si, sum(vp))\n",
    "\t\n",
    "}\n",
    "\n",
    "# main program\n",
    "\n",
    "#make sure M.txt is the data file to be executed \n",
    "#input the number of unique node and assign it to n; 4 is the default for M.txt data; \n",
    "#n should be the unique count of the \"to website\" column; a word count \n",
    "#the following initial values are for Example 5.1 of the Mining Massive Data book (1 or 2nd edition)\n",
    "library(rmr2)\n",
    "\n",
    "n<-714546 # Number of unique nodes\n",
    "\n",
    "#Final Page Rank once multiple iterations have processed\n",
    "v1<- rep(500000/n, n) #Final Page Rank once multiple iterations have processed\n",
    "\t\t\t\t #Note that v1 is a global varialbe to be used in the while loop below and the pgij function above\n",
    "\n",
    "b<- 0.85 #Weighting value to prevent v1 from converging to zero\n",
    "i<-1\n",
    "nn<-10 #Amount of iterations, 50-75 for big data files\n",
    "input.dfs<-(\"/user/cloudera/webr2.txt\")\n",
    "\n",
    "# the loop to start iterations\n",
    "while (i<=nn){\n",
    "\t\n",
    "\t#MapReduce function from dfs \n",
    "\ttv<-from.dfs(mapreduce(input=input.dfs, input.format=\"text\", map=pgij, reduce =pgij2))\n",
    "\tv1<-as.numeric(tv$val) #here the output in tv is passed onto the global variable v1\n",
    "\ti <- i+1\n",
    "}\n",
    "#Save output into text file and also print in command terminal\n",
    "write.table(data.frame(tv$key,v1), \"/home/cloudera/PageRank_G50I.txt\", sep= \" \", col.names= FALSE, row.names= FALSE)\n",
    "print(paste(\"The final printout in the $key part is \", tv$key))\n",
    "print(paste(\"The final page rank vector is \", v1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is code modified from pageRankM.r for the 'web-Google.txt' dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select * from pagerank50i where rank IS NOT NULL order by rank;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an Impala query to help identify the top page ranks that we found earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"AppendixA\"></a> Appendix A (Code Summary)\n",
    "pageRankM.r code (pageRankG.r mirrors this very closely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(rmr2): there is no package called 'rmr2'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(rmr2): there is no package called 'rmr2'\nTraceback:\n",
      "1. library(rmr2)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "#pageRankM.r\n",
    "#Author: Kevin Nguyen\n",
    "#A10: Big Data Applications using Map Reduce\n",
    "#17 March 2017\n",
    "#\n",
    "#Template provided by Dr. Shing Chang\n",
    "#\n",
    "\n",
    "#a map reduce code to iteratively compute v=M v where columuns of M adds to 1\n",
    "#an example on section 5.2 in Mining of Massive Datasets\n",
    "#M is n x n and v is n x 1\n",
    "#M is in the format of (i, j, mij), sum(mij)=1 over j\n",
    "\n",
    "# mapper function\n",
    "\t#Note that the input part of mapreduce will provide k as NULL and v as the content of M in (i, j, mij format) \n",
    "pgij<-function(k,v){\n",
    "\t\n",
    "\t#Seperates v into usable components\n",
    "\ta<-strsplit(unlist(v), \" \")#Splits the unlisted v into subgroups \n",
    "\tsi<-as.numeric(sapply(a, \"[\", 1))#Seperate subelement used as key into vector (i)\n",
    "\tvp<-as.numeric(sapply(a, \"[\", 3))# Seperate subelement used as values into vector (mij)\n",
    "\t\n",
    "\t#The key passes into the reduce function is si and value vp\n",
    "\tkeyval(si, vp)\n",
    "}\n",
    "\n",
    "# reducer function\n",
    "pgij2<-function(si,vp){\n",
    "\t\n",
    "\t#Page Rank calculation v = M*v where v1 converges to the final page rank after multiple iterations\n",
    "\tvf<-vp*v1\n",
    "\t\n",
    "\t#Summation of values with respect to their key for final page rank vector\n",
    "\tkeyval(si, sum(vf))\n",
    "}\n",
    "\n",
    "# main program\n",
    "\n",
    "#make sure M.txt is the data file to be executed \n",
    "#input the number of unique node and assign it to n; 4 is the default for M.txt data; \n",
    "#n should be the unique count of the \"to website\" column; a word count \n",
    "#the following initial values are for Example 5.1 of the Mining Massive Data book (1 or 2nd edition)\n",
    "library(rmr2)\n",
    "\n",
    "n<-4 # Number of unique nodes\n",
    "\n",
    "#Final Page Rank once multiple iterations have processed\n",
    "v1<- rep(1/n, n) #Final Page Rank once multiple iterations have processed\n",
    "\t\t\t\t #Note that v1 is a global varialbe to be used in the while loop below and the pgij function above\n",
    "i=1\n",
    "nn<-10 #Amount of iterations, 50-75 for big data files\n",
    "input.dfs<-(\"/user/cloudera/M.txt\")\n",
    "\n",
    "# the loop to start iterations\n",
    "while (i<=nn){\n",
    "\t\n",
    "\t#MapReduce function from dfs \n",
    "\ttv<-from.dfs(mapreduce(input=input.dfs, input.format=\"text\", map=pgij, reduce = pgij2))\n",
    "\tv1<-as.numeric(tv$val) #here the output in tv is passed onto the global variable v1\n",
    "\ti <- i+1\n",
    "}\n",
    "#Save output into text file and also print in command terminal\n",
    "write.table(data.frame(tv$key,v1), \"/home/cloudera/PageRank_4I.txt\", sep= \" \")\n",
    "print(paste(\"The final printout in the $key part is \", tv$key))\n",
    "print(paste(\"The final page rank vector is \", v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is a detailed explanation of the code above chunk by chunk in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapper function\n",
    "\t#Note that the input part of mapreduce will provide k as NULL and v as the content of M in (i, j, mij format) \n",
    "pgij<-function(k,v){\n",
    "\t\n",
    "\t#Seperates v into usable components\n",
    "\ta<-strsplit(unlist(v), \" \")#Splits the unlisted v\n",
    "\tsi<-as.numeric(sapply(a, \"[\", 1))#Seperate subelement used as key into vector (i)\n",
    "\tvp<-as.numeric(sapply(a, \"[\", 3))# Seperate subelement used as values into vector (mij)\n",
    "\t\n",
    "\t#The key passes into the reduce function is si and value vp\n",
    "\tkeyval(si, vp)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper pgij has a keyval(k = null, v = M.txt). The mapper 'maps' the your value to its respective key you assign. Data manipulation can be done in this section. In this case, the mapper is used to extract our M in a usable manner to be sent to the reducer where the page rank vector is calculated.\n",
    "\n",
    "The content of v is unlisted and is then splitted into individual characters (M.txt is space delimited). after the first character of each subgroup is extracted into a column using sapply based on their position. The first character represents our i and the 3rd character represents our Mij in the i j Mij format of M.txt. These characters are of each subgroup are extracted using the sapply function. We convert the characters to numerical values using `as.numeric` and then store them into si and vp. Our new keyval(si, vp) are now ready to be passed into the reducer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducer function\n",
    "pgij2<-function(si,vp){\n",
    "\t\n",
    "\t#Page Rank calculation v = M*v where v1 converges to the final page rank after multiple iterations\n",
    "\tvf<-vp*v1\n",
    "\t\n",
    "\t#Summation of values with respect to their key for final page rank vector\n",
    "\tkeyval(si, sum(vf))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer's keyval' contains the contant that was passed from the mapper. Due to the key and the sequence being preserved, vp (Mij) is multiplied by our v1 which is a global variable used to help iterate the page rank vector and is stored in vf. In our new keyval(si, sum(vf)), the contents of vf will be summed based on their associated key, si. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(rmr2): there is no package called 'rmr2'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(rmr2): there is no package called 'rmr2'\nTraceback:\n",
      "1. library(rmr2)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(rmr2)\n",
    "\n",
    "n<-4 # Number of unique nodes\n",
    "\n",
    "#Final Page Rank once multiple iterations have processed\n",
    "v1<- rep(1/n, n) #Final Page Rank once multiple iterations have processed\n",
    "\t\t\t\t #Note that v1 is a global varialbe to be used in the while loop below and the pgij function above\n",
    "i=1\n",
    "nn<-4 #Amount of iterations, 50-75 for big data files\n",
    "input.dfs<-(\"/user/cloudera/M.txt\")\n",
    "\n",
    "# the loop to start iterations\n",
    "while (i<=nn){\n",
    "\t\n",
    "\t#MapReduce function from dfs \n",
    "\ttv<-from.dfs(mapreduce(input=input.dfs, input.format=\"text\", map=pgij, reduce = pgij2))\n",
    "\tv1<-as.numeric(tv$val) #here the output in tv is passed onto the global variable v1\n",
    "\ti <- i+1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above are the intializations required to start our mapreduce job. The mapreduce statement calls our mapper and reducer functions, pgij and pgij2. The input of the mapreduce statement requires a big data object. For the input we provide the path to our M.txt file stored in HDFS, which is used as the content of v in our mapper function. The val of the output is then stored into our global variable v1 which is used to store our current iteration of the final page rank vector. The mapreduce job is ran in a loop for some 'nn' iterations based on what the user's needs. \n",
    "\n",
    "The rmr2 library is used to help debug by helping us print within the mapreduce functions as the statement is ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in data.frame(tv$key, v1): object 'tv' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in data.frame(tv$key, v1): object 'tv' not found\nTraceback:\n",
      "1. write.table(data.frame(tv$key, v1), \"/home/cloudera/PageRank_4I.txt\", \n .     sep = \" \")",
      "2. is.data.frame(x)",
      "3. data.frame(tv$key, v1)"
     ]
    }
   ],
   "source": [
    "#Save output into text file and also print in command terminal\n",
    "write.table(data.frame(tv$key,v1), \"/home/cloudera/PageRank_4I.txt\", sep= \" \")\n",
    "print(paste(\"The final printout in the $key part is \", tv$key))\n",
    "print(paste(\"The final page rank vector is \", v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the mapreduce job is performed, we save the results into a text file that is space delimited in our cloudera home directory. The key and final page rank vector are also printed out for easy viewing of the results. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
