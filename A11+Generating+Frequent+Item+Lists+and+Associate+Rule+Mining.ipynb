{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A11: Generating Frequent Item Lists and Associate Rule Mining \n",
    "##### by Kevin Nguyen (3 April 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Associate rules are about analyzing the likelihood if one item or action leads directly into a different item or action. This assignment will be based on a Market Basket Analysis. A Market Basket Analysis is based on a theory that if you buy a certain group of items, you are more or less likely to buy another group of items. \n",
    "\n",
    "We will be analyzing the contents within ‘bookdata.tsv.gz’ using R and the module arules. \n",
    "\n",
    "In this assignment, we will be answering each question below.\n",
    "\n",
    "1. Discuss how you formulate this problem in terms of transactions and items for an associate rule mining analysis. Specifically, what are transactions and what are items in this data set? (Hint: A rule of interest is X -> Y where X and Y are book titles with enough support and confidence.)\n",
    "\n",
    "2. Run Apriori algorithm with the support of 0.002 and confidence .75. Sort all the rules that meet this criterion according to its support and then confidence. Identify the top five rules. Interpret the results.\n",
    "\n",
    "3. Among the top five rules listed in the previous question, examine which rules are for real and which ones are by chance by using a measure called lift. Define your lift criterion and discuss the decision you made.\n",
    "\n",
    "4. Run Eclat on the same problem. Discuss your findings. What are main differences between Eclat and Apriori?\n",
    "\n",
    "5. Outline your big data strategy for this data if you are asked to run associate rule mining on Amazon data set when the main memory of a computer can not fit all records in this data set.\n",
    "\n",
    "6. (5 points bonus) List the top five most read books in this data set. Describe how you come to this list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "Discuss how you formulate this problem in terms of transactions and items for an associate rule mining analysis. Specifically, what are transactions and what are items in this data set? (Hint: A rule of interest is X -> Y where X and Y are book titles with enough support and confidence.)\n",
    "\n",
    "The bookdata.tsv.gz contains data of books that users have rated. In this problem, we are trying to identify a link between books, e.g. if a user buys book 1 then the user buys book 2. In terms of this problem the ‘transactions’ are the user id’s and the ‘items’ within that data set are the actual books they have purchased. With such a large file, we will need to use the Apriori algorithm to help us narrow down the rules that will meet a certain criterion we specify. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "Run Apriori algorithm with the support of 0.002 and confidence .75. Sort all the rules that meet this criterion according to its support and then confidence. Identify the top five rules. Interpret the results.\n",
    "\n",
    "The top five results of the Apriori algorithm are listed below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 Results sorted by Support (\"Rule, Support, Confidence, Lift\")  \n",
    "\n",
    "1. \"{Harry Potter and the Prisoner of Azkaban,Harry Potter and the Sorcerer's Stone} => {Harry Potter and the Chamber of Secrets}\"\t\n",
    "0.00258392322056716\t0.894736842105263\t146.641318598989\n",
    "\n",
    "2. \"{Harry Potter and the Chamber of Secrets,Harry Potter and the Prisoner of Azkaban} => {Harry Potter and the Sorcerer's Stone}\"\t\n",
    "0.00258392322056716\t0.835087719298246\t92.4498313090419\n",
    "\n",
    "3. \"{Harry Potter and the Chamber of Secrets,Harry Potter and the Goblet of Fire} => {Harry Potter and the Prisoner of Azkaban}\"\n",
    "0.00238850045598645\t0.897959183673469\t183.390741662519\n",
    "\n",
    "4.  \"{Harry Potter and the Goblet of Fire,Harry Potter and the Prisoner of Azkaban} => {Harry Potter and the Chamber of Secrets}\"\t\n",
    "0.00238850045598645\t0.883534136546185\t144.805270905687\n",
    "\n",
    "5.  \"{Harry Potter and the Chamber of Secrets,Harry Potter and the Prisoner of Azkaban} => {Harry Potter and the Goblet of Fire}\"\t\n",
    "0.00238850045598645\t0.771929824561403\t182.310031488979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 rules contain books from the Harry Potter series. The first two rules have the same support due to the two rules being nearly identical where 'if book 1 & 2, then book 3' will have the same support as 'if book 3 & 1, then book 2'. This information, while still useful, shows us the same general idea.  \n",
    "The same issue can be seen in rule 3, 4, and 5 where the support is the exact same due to the order in which the books are interchanged within the exact same rule set.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Top 5 Results sorted by Confidence (\"Rule, Support, Confidence, Lift\")  \n",
    "\n",
    "1. \"{Harry Potter and the Goblet of Fire,Harry Potter and the Prisoner of Azkaban,Harry Potter and the Sorcerer's Stone} => {Harry Potter and the Chamber of Secrets}\"\t  \n",
    "0.00209536630911539\t0.965\t158.156975088968\n",
    "  \n",
    "2. \"{Harry Potter and the Chamber of Secrets,Harry Potter and the Goblet of Fire,Harry Potter and the Sorcerer's Stone} => {Harry Potter and the Prisoner of Azkaban}\"\t  \n",
    "0.00209536630911539\t0.932367149758454\t190.417901175059\n",
    "  \n",
    "3. \"{Harry Potter and the Goblet of Fire,Harry Potter and the Sorcerer's Stone} => {Harry Potter and the Chamber of Secrets}\"  \t \n",
    "0.00224736179267816\t0.924107142857143\t151.454912303\n",
    "  \n",
    "4. \"{Harry Potter and the Chamber of Secrets,Harry Potter and the Goblet of Fire} => {Harry Potter and the Prisoner of Azkaban}\"  \n",
    "0.00238850045598645\t0.897959183673469\t183.390741662519\n",
    "  \n",
    "5. \"{Harry Potter and the Prisoner of Azkaban,Harry Potter and the Sorcerer's Stone} => {Harry Potter and the Chamber of Secrets}\"\t    \n",
    "0.00258392322056716\t0.894736842105263\t146.641318598989\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the top 5 rules contain books from the Harry Potter series.  \n",
    "The same issue from the previous top 5 rules sorted by support are also present for confidence. All the rules tell the same story even though the confidence values are not exactly the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R code used to run the apriori algorithm and sort the rules based on support and confidence are in Appendix A at the end of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \n",
    "Among the top five rules listed in the previous question, examine which rules are for real and which ones are by chance by using a measure called lift. Define your lift criterion and discuss the decision you made.\n",
    "\n",
    "Lift is is a ratio of confidence stating whether the rule is likely real and true or if it was created by chance.  \n",
    "\n",
    "Lift Criterion  \n",
    "*Lift > 1 : Indicates a rule is **USEFUL** in finding consequent item sets as opposed to selecting transactions randomly*   \n",
    "*Lift < 1 : Indicates a rule is **LESS USEFUL** in finding consequent item sets as opposed to selecting transactions randomly*  \n",
    "Lift = confidence of rule / confidence of consequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing both the rules sorted by confidence, all rules generated are all real due to having a lift >> 1. Rules that are created by chance are likely to have a lift that is very low in comparison to the lift's within other rules.  \n",
    "All five rules are real but they are all variations of the same thing in essence. Had the rules been less than 1, then you could safely conclude that they were created by chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Run Eclat on the same problem. Discuss your findings. What are main differences between Eclat and Apriori?\n",
    "\n",
    "When running Eclat, the output is the most frequent itemsets based on the support that you set. No rules are generated using Eclat by itself. In order to generate rules, the function 'ruleInduction' must be used to extract the rules out of the output from Eclat. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eclat (support = 0.002)  \n",
    "Output = All itemsets with support above 0.002 are listed.  \n",
    "Eclat (support = 0.002, minlen = 2)\n",
    "Output = All itemsets with atleast 2 items with a support above 0.002 are listed. \n",
    "Note: The R code used to generate the Eclat can be viewed in Appendix B.\n",
    "\n",
    "One of the itemsets generated by Eclat contained 3 items, Harry Potter and the Chamber of Secrets,Harry Potter and the Prisoner of Azkaban,Harry Potter and the Sorcerer's Stone. This itemset was unique and only appeared once. While, in the rules generated by Apriori, the 3 items above appeared 3 seperate times with the same support in a different rule order. Eclat in this instance helped reduce redundant information. \n",
    "\n",
    "If the 'ruleInduction' function was used on the new eclat itemset, then you would get a different top 5 list based on support and confidence then that of Apriori due to the lack of 'variations' of essentially the same rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between Apriori and Eclat are listed as the following.\n",
    "1. Apriori outputs rules based on your parameters as opposed to the itemsets from Eclat\n",
    "2. Rules generated by Eclat require the use of the function 'ruleInduction' while Apriori outputs rules by default\n",
    "3. Apriori scans the dataset multiple times as opposed to a single time by Eclat\n",
    "4. Eclat is faster than Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Outline your big data strategy for this data if you are asked to run associate rule mining on Amazon data set when the main memory of a computer can not fit all records in this data set. \n",
    "\n",
    "First we must consider the limitations given and the subsequent consequences of those limitations. If the entirety of the data set cannot fit in the main memory then there will be some complications. For instance, transactions that would correlate with each other get chunked out into different subsections and may not pass certain parameters, thus when regrouped that data is potentionally lost. \n",
    "\n",
    "Below are the steps I would use in terms of my big data strategy.  \n",
    "\n",
    "1. Seperate the entire dataset into chunks. \n",
    "2. Run either Eclat or Apriori with less strict parameters then what you are looking for\n",
    "3. Filter out any transactions that do not satisfy those parameters\n",
    "4. Recombine results \n",
    "5. Repeat steps 2-4 until you can fit all the data into main memory\n",
    "6. Run Eclat or Apriori with expected parameters \n",
    "7. Analyze results  \n",
    "\n",
    "Some issues with this approach is time and efficiency depending on the size of the data. Naturally some data will be lost using this method but hopefully with the less strict parameters we can preserve most of it.\n",
    "Multiple runs while chunking the dataset could also be beneficial in retaining some of the lost data from other runs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "List the top five most read books in this data set. Describe how you come to this list.\n",
    "\n",
    "![](files/top5readbooks.PNG)\n",
    "\n",
    "The top 5 books are as the following. \n",
    "\n",
    "1. \"Wild Animus\"\n",
    "2. \"The Lovely Bones: A Novel\"\n",
    "3. \"She's Come Undone\"\n",
    "4. \"The Da Vinci Code\"\n",
    "5. \"Harry Potter and the Sorcerer's Stone\"\n",
    "\n",
    "This list was generated by using a function within arules called 'itemFrequencyPlot'. This function allows us to count the frequency of books and filters out everything but the top 5. We originally tried to use the function 'itemFrequency' but there were no parameters to filter the results. \n",
    "\n",
    "The top 5 were generated with the following R code below. The code can also be viewed in Appendix C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(arules)\n",
    "bookbaskets<-read.transactions(“/home/cloudera/bookdata.tsv.gz”, format=”single”, sep=”\\t”, cols=c(“userid”, “title”), rm.duplicates=T)\n",
    "itemFrequencyPlot(bookbaskets, topN=5, type=\"absolute\")\n",
    "#For the plot to show up, the code must be ran in the command terminal in cloudera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment we utilized two common Association Rule Mining algorithms, Apriori and Eclat. Apriori generated rules while Eclat generates itemsets. This assignment was the first case of examining the parameter, lift, when helping us analyze whether certain rules were true or occured by chance. Additionally, issues using Apriori and Eclat were also exposed when looking at how to apply them to datasets that could not fit into main memory in its entirety. Lastly, the top five most read books were generated. All of this was accomplished in RHadoop in the cloudera environment using the 'arules' module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(arules)\n",
    "bookbaskets<-read.transactions(“/home/cloudera/bookdata.tsv.gz”, format=”single”, sep=”\\t”, cols=c(“userid”, “title”), rm.duplicates=T)\n",
    "apr<-apriori(bookbaskets, parameter = list(supp= 0.002, conf= 0.75, target = \"rules\"))\n",
    "summary(apr)\n",
    "aprS<-sort(apr, by= \"support\", decreasing = TRUE)\n",
    "aprC<-sort(apr, by = \"confidence\", decreasing = TRUE)\n",
    "aprSF<-as(aprSF, \"data.frame\")\n",
    "aprCF<-as(aprCF, \"data.frame\")\n",
    "write.table(aprSF, file = \"aprS.txt\", sep = \"\\t\", row.names = FALSE)\n",
    "write.table(aprCF, file = \"aprC.txt\", sep = \"\\t\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(arules)\n",
    "bookbaskets<-read.transactions(“/home/cloudera/bookdata.tsv.gz”, format=”single”, sep=”\\t”, cols=c(“userid”, “title”), rm.duplicates=T)\n",
    "eclItemSet<-eclat(bookbaskets, parameter = list(supp=0.1, minlen=2))\n",
    "eclSort<-sort(eclItemSet, by= \"support\", decreasing = TRUE)\n",
    "eclSortF<-as(eclSort, \"data.frame\")\n",
    "write.table(eclSortF, file = \"eclOutput.txt\", sep = \"\\t\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(arules)\n",
    "bookbaskets<-read.transactions(“/home/cloudera/bookdata.tsv.gz”, format=”single”, sep=”\\t”, cols=c(“userid”, “title”), rm.duplicates=T)\n",
    "itemFrequencyPlot(bookbaskets, topN=5, type=\"absolute\")\n",
    "#For the plot to show up, the code must be ran in the command terminal in cloudera."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
